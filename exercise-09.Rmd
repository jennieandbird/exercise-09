---
title: "Exercise 09"
author: "Jennie"
date: "2024-03-07"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Step 1

-   Using the {tidyverse} `read_csv()` function, load the “Street_et_al_2017.csv” dataset from [this URL](https://raw.githubusercontent.com/difiore/ada-2024-datasets/main/Street_et_al_2017.csv) as a “tibble” named **d**.

-   Do a quick exploratory data analysis where you generate the five-number summary (median, minimum and maximum and 1st and 3rd quartile values), plus mean and standard deviation, for each quantitative variable.

> **HINT**: The `skim()` function from the package {skimr} makes this very easy!

```{r Step1}
library(tidyverse)
library(skimr)
library(broom)
library(infer)
d <- read_csv("https://raw.githubusercontent.com/difiore/ada-2024-datasets/main/Street_et_al_2017.csv")
as_tibble(d)
s <- skim(d)
summary <- s |> filter(skim_type == "numeric") |> rename(variable = skim_variable, missing = n_missing, mean = numeric.mean, sd = numeric.sd, min = numeric.p0, p25 = numeric.p25, median = numeric.p50, p75 = numeric.p75, max = numeric.p100) |> select(variable, missing, mean, sd, min, p25, median, p75, max)
```

# Step 2

-   From this dataset, plot brain size (**ECV**) as a function of social group size (**Group_size**), longevity (**Longevity**), juvenile period length (**Weaning**), and reproductive lifespan (**Repro_lifespan**).

```{r Step2}
library(ggplot2)
ggplot(data = d, aes(y = ECV, x = Group_size)) + geom_point()
ggplot(data = d, aes(y = ECV, x = Longevity)) + geom_point()
ggplot(data = d, aes(y = ECV, x = Weaning)) + geom_point()
ggplot(data = d, aes(y = ECV, x = Repro_lifespan)) + geom_point()
```

# Step 3

-   Derive by hand the ordinary least squares regression coefficients β1 and β0 for ECV as a function of social group size.

> **HINT**: You will need to remove rows from your dataset where one of these variables is missing.

```{r Step3}
d <- d |> filter_at(vars(ECV,Group_size),all_vars(!is.na(.)))
beta1 <- cor(d$Group_size, d$ECV) * (sd(d$ECV)/sd(d$Group_size))
beta0 <- mean(d$ECV) - beta1 * mean(d$Group_size)
```

β1: 2.46

β0: 30.36

# Step 4

-   Confirm that you get the same results using the `lm()` function.

```{r Step4}
m <- lm(ECV ~ Group_size, data = d)
```

β1: 2.46

β0: 30.36

# Step 5

-   Repeat the analysis above for three different major radiations of primates - “catarrhines”, “platyrrhines”, and “strepsirhines”) separately. These are stored in the variable **Taxonomic_group**. Do your regression coefficients differ among groups? How might you determine this?

```{r Step5}
d.group <- d |> filter_at(vars(ECV,Group_size),all_vars(!is.na(.))) |> group_by(Taxonomic_group) |> summarize(beta1 = (cor(Group_size, ECV) * (sd(ECV)/sd(Group_size))), beta0 = (mean(ECV) - beta1 * mean(Group_size)))
```

I could permute data and apply linear modeling to determine whether the regression coefficients differ among groups.

# Step 6

-   For your first regression of ECV on social group size, calculate the standard error for the slope coefficient, the 95% CI, and the *p* value associated with this coefficient by hand. Also extract this same information from the results of running the `lm()` function.

```{r Step6}
# setting up Anova table
SSY <- sum((m$model$ECV - mean(m$model$ECV))^2)
SSR <- sum((m$fitted.values - mean(m$model$ECV))^2)
SSE <- sum((m$model$ECV - m$fitted.values)^2)
DFR <- 1
DFE <- nrow(d) - DFR - 1
DFY <- nrow(d) - DFR
MSR <- SSR/DFR
MSE <- SSE/DFE
MSY <- SSY/DFY
# Standard error for the slope coefficients (SEbeta1 and SEbeta0)
SSX <- sum((m$model$Group_size - mean(m$model$Group_size))^2)
SEbeta1 <- sqrt(MSE/SSX)
SEbeta0 <- sqrt((MSE * sum(m$model$Group_size^2))/(151 * SSX))
SEbeta0
# 95% CI
alpha <- 0.05
m.summary <- tidy(m)
m.summary$calc.statistic <- (m.summary$estimate - 0)/m.summary$std.error
lower <- m.summary$estimate - qt(1 - alpha/2, df = nrow(d) - 2) * m.summary$std.error
upper <- m.summary$estimate + qt(1 - alpha/2, df = nrow(d) - 2) * m.summary$std.error
CI <- cbind(lower, upper)
# p value
m.summary$calc.p.value <- 2 * pt(m.summary$calc.statistic, df = nrow(d) - 2, lower.tail = FALSE)
```

Standard error for slope coefficients: SEβ1 is 0.35, SEβ0 is 6.80. `summary(m)` shows 0.3508 and 6.7963.

95% CI: 16.93, 43.79; 1.77, 3.16. `confint(m, level = 1 - alpha)` shows 16.926896, 43.786153; 1.769874, 3.156269.

*p* value: 1.56 e-05, 7.26 e-11 `summary(m)` shows 1.56e-05 and 7.26e-11.

# Step 7

-   Use a permutation approach with 1000 permutations to generate a null sampling distribution for the **slope coefficient**. What is it that you need to permute? What is the p value associated with your original slope coefficient? You can use either the percentile method (i.e., using quantiles from the actual permutation-based null sampling distribution) or a theory-based method (i.e., using the standard deviation of the permutation-based null sampling distribution as the estimate of the standard error, along with a normal or t distribution), or both, to calculate this p value.

```{r Step7}
critical_value <- qt((1 - (alpha/2)), df = (149))
# null sampling distibution
permuted.slope <- d |> specify(ECV ~ Group_size) |> hypothesize(null = "independence") |> generate(reps = 1000, type = "permute") |> calculate(stat = "slope")
permuted.slope.summary <- permuted.slope |> summarize(estimate = mean(stat), std.error = sd(stat), lower = estimate - std.error * critical_value, upper = estimate + std.error * critical_value, perm.lower = quantile(stat, 0.025), perm.upper = quantile(stat, 0.975))
# compare with the original slope
original.slope <- lm(data = d, ECV ~ Group_size) |> tidy(conf.int = TRUE, conf.level = 0.95) |> mutate(lower = estimate - std.error * critical_value, upper = estimate + std.error * critical_value) |> filter(term == "Group_size")
# get p value
(p.value <- permuted.slope |> get_p_value(obs_stat = original.slope$estimate, direction="two_sided"))
```

The permutation will shuffle the values of Group Size and ECV as though they are unrelated. The *p* value for the original slope was 7.26e-11. None of the permuted simulations yielded a slope estimate as great or greater than the one observed in the original slope, and the *p* value is 0.

# Step 8

-   Use bootstrapping to generate a 95% CI for your estimate of the slope coefficient using both the percentile method and the theory-based method (i.e., using on the standard deviation of the bootstrapped sampling distribution as an estimate of the standard error). Do these CIs suggest that your slope coefficient is different from zero?

```{r Step8}
# bootstrapping sampling distribution
boot.slope <- d |> specify(ECV ~ Group_size) |> generate(reps = 1000, type = "bootstrap") |> calculate(stat = "slope")
boot.slope.summary <- boot.slope |> summarize(estimate = mean(stat), std.error = sd(stat), lower = estimate - std.error * critical_value, upper = estimate + std.error * critical_value, boot.lower = quantile(stat, 0.025), boot.upper = quantile(stat, 0.975))
# percentile method
CI.percentile <- get_ci(boot.slope, level = 1 - alpha, type = "percentile")
# theory based method
CI.theory <- get_ci(boot.slope, level = 1 - alpha, type = "se", point_estimate = pull(boot.slope.summary,
    estimate))

# is it different from zero?
t_stat <- t.test(x = boot.slope.summary, mu = boot.slope.summary$estimate, alternative = "greater")
```

The p value for the bootstrapped distribution is above 0.05 and the slope coefficient is not significantly different from zero.
